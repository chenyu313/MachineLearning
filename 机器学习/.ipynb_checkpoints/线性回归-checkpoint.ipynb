{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义：利⽤回归⽅程(函数)对⼀个或多个⾃变量(特征值)和因变量(⽬标值)之间关系进⾏建模的⼀种分析⽅式\n",
    "- 线性回归的分类：\n",
    "    - 线性关系\n",
    "    - 非线性关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归api初步使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h(w)=w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{3} \\ldots+\\mathrm{b}=w^{T} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
    "    - 参数：\n",
    "        - fit_intercept：是否计算偏置\n",
    "    - 属性：\n",
    "        - fit_intercept：是否计算偏置\n",
    "        - LinearRegression.coef_：回归系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 获取数据集\n",
    "- 数据基本处理（该案例中省略）\n",
    "- 特征工程（该案例中省略）\n",
    "- 机器学习\n",
    "- 模型评估（该案例中省略）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性回归的系数是：\n",
      " [0.3 0.7]\n",
      "线性回归的预测结果是：\n",
      " [86.]\n"
     ]
    }
   ],
   "source": [
    "#导入模块\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#构造数据集\n",
    "x = [[80, 86],\n",
    "    [82, 80],\n",
    "    [85, 78],\n",
    "    [90, 90],\n",
    "    [86, 82],\n",
    "    [82, 90],\n",
    "    [78, 80],\n",
    "    [92, 94]]\n",
    "y = [84.2, 80.6, 80.1, 90, 83.2, 87.6, 79.4, 93.4]\n",
    "\n",
    "#机器学习-模型训练\n",
    "\n",
    "#实例化api\n",
    "estimator=LinearRegression()\n",
    "#使用fit方法进行训练\n",
    "estimator.fit(x,y)\n",
    "#打印对应系数\n",
    "print(\"线性回归的系数是：\\n\",estimator.coef_)\n",
    "#打印预测结果\n",
    "print(\"线性回归的预测结果是：\\n\",estimator.predict([[100,80]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归的损失和优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当真实结果与我们预测的结果存在一定误差时，我们需要将这个误差衡量出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{J}(\\mathrm{w}) &=\\left(\\mathrm{h}\\left(x_{1}\\right)-y_{1}\\right)^{2}+\\left(\\mathrm{h}\\left(x_{2}\\right)-y_{2}\\right)^{2}+\\cdots+\\left(\\mathrm{h}\\left(x_{m}\\right)-y_{m}\\right)^{2} \\\\\n",
    "&=\\sum_{i=1}^{m}\\left(\\mathrm{~h}\\left(x_{i}\\right)-y_{i}\\right)^{2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- yi为第i个训练样本的真实值\n",
    "- h(xi)为第i个训练样本特征值组合预测函数\n",
    "- 又称最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的是找到最小损失对应的W值（权重）  \n",
    "- 线性回归经常使用的两种优化算法\n",
    "    - 正规方程\n",
    "    - 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正规方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w=\\left(X^{T} X\\right)^{-1} X^{T} y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果。  \n",
    "缺点：当特征过多过于复杂时，求解速度太慢且得不到结果。  \n",
    "（正规方程一步到位，梯度下降一步一步来）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{i+1}=\\theta_{i}-\\alpha \\frac{\\partial}{\\partial \\theta_{i}} J(\\theta)\n",
    "$$\n",
    "- 在单变量函数中，梯度其实就是函数的微分，代表着函数在某个给定点切线的斜率；\n",
    "- 在多变量函数中，梯度其实是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；\n",
    "- α在梯度下降算法中被称作为学习率或者步⻓；\n",
    "- 梯度前加⼀个负号，代表着是在下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正规方程与梯度下降对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 梯度下降 |正规方程 |\n",
    "| --- | --- |\n",
    "| 需要选择学习率 | 不需要 |\n",
    "| 需要迭代求解 | 一次运算得出 |\n",
    "|特征数量较大可以使用 | 需要计算方程，时间复杂度O(n3) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法选择依据：\n",
    "- 小规模数据：\n",
    "    - 正规方程：LinearRegression(不能解决拟合问题)\n",
    "    - 岭回归\n",
    "- 大规模数据：\n",
    "    - 梯度下降：SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 梯度下降算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- α（步长）：决定了在梯度下降迭代的过程中，每⼀步沿梯度负⽅向前进的⻓度。\n",
    "- x（特征）：指的是样本中输⼊部分。\n",
    "- h (x) = θ + θ x（特征函数）：在监督学习中，为了拟合输⼊样本，⽽使⽤的假设函数，记为h (x)。\n",
    "- 损失函数：为了评估模型拟合的好坏，通常⽤损失函数来度量拟合的程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法算法\n",
    "- 全梯度下降算法(Full gradient descent),\n",
    "- 随机梯度下降算法(Stochastic gradient descent),\n",
    "- ⼩批量梯度下降算法(Mini-batch gradient descent),\n",
    "- 随机平均梯度下降算法(Stochastic average gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全梯度下降算法(FG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{i}=\\theta_{i}-\\alpha \\sum_{j=1}^{m}\\left(h_{\\theta}\\left(x_{0}^{(j)}, x_{1}^{(j)}, \\ldots x_{n}^{(j)}\\right)-y_{j}\\right) x_{i}^{(j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 批量梯度下降法，具体做法也就是在更新参数时使⽤所有的样本来进⾏更新\n",
    "- 计算训练集所有样本误差，对其求和再取平均值作为⽬标函数\n",
    "- 因为计算整个数据集上的梯度，故速度会很慢\n",
    "- 批梯度下降法同样也不能在线更新模型，即在运⾏的过程中，不能增加新的样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降算法(SG)\n",
    "$$\n",
    "\\theta_{i}=\\theta_{i}-\\alpha\\left(h_{\\theta}\\left(x_{0}^{(j)}, x_{1}^{(j)}, \\ldots x_{n}^{(j)}\\right)-y_{j}\\right) x_{i}^{(j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其每轮计算的⽬标函数不再是全体样本误差，⽽仅是单个样本误差，即每次只代⼊计算⼀个样本⽬标函数的梯度来更新权重，再取下⼀个样本重复此过程，直到损失函数值停⽌下降或损失函数值⼩于某个可以容忍的阈值。  \n",
    "&emsp;&emsp;但是由于，SG每次只使⽤⼀个样本迭代，若遇上噪声则容易陷⼊局部最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⼩批量梯度下降算法(mini-batch)\n",
    "$$\n",
    "\\theta_{i}=\\theta_{i}-\\alpha \\sum_{j=t}^{t+x-1}\\left(h_{\\theta}\\left(x_{0}^{(j)}, x_{1}^{(j)}, \\ldots x_{n}^{(j)}\\right)-y_{j}\\right) x_{i}^{(j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;每次从训练样本集上随机抽取⼀个⼩样本集，在抽出来的⼩样本集上采⽤FG迭代更新权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  随机平均梯度下降算法(SAG)\n",
    "$$\n",
    "\\theta_{i}=\\theta_{i}-\\frac{\\alpha}{n}\\left(h_{\\theta}\\left(x_{0}^{(j)}, x_{1}^{(j)}, \\ldots x_{n}^{(j)}\\right)-y_{j}\\right) x_{i}^{(j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;会给每个样本都维持⼀个平均值,后期计算的时候,参考这个平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### api使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正规方程：\n",
    "    - sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
    "    - 参数：\n",
    "        - fit_intercept：是否计算偏置\n",
    "    - 属性：\n",
    "        - LinearRegression.coef_：回归系数\n",
    "        - LinearRegression.intercept_：偏置\n",
    "\n",
    "\n",
    "- 梯度下降：\n",
    "    - sklearn.linear_model.SGDRegressor(loss=\"squared_loss\", fit_intercept=True, learning_rate ='invscaling',eta0=0.01)\n",
    "    - 定义：实现了随机梯度下降学习，它⽀持不同的loss函数和正则化惩罚项来拟合线性回归模型\n",
    "    - 参数：\n",
    "        - loss:损失类型\n",
    "            - loss=”squared_loss”: 普通最⼩⼆乘法\n",
    "        - fit_intercept：是否计算偏置\n",
    "        - learning_rate : string, optional\n",
    "            - 学习率填充\n",
    "            - 'constant': eta = eta0\n",
    "            - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]\n",
    "            - 'invscaling': eta = eta0 / pow(t, power_t)\n",
    "            - 对于⼀个常数值的学习率来说，可以使⽤learning_rate=’constant’ ，并使⽤eta0来指定学习率\n",
    "     - 属性：\n",
    "         - SGDRegressor.coef_：回归系数\n",
    "         - SGDRegressor.intercept_：偏置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 案例：波士顿房价预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本步骤\n",
    "- 获取数据\n",
    "- 数据基本处理---分割数据\n",
    "- 特征工程---标准化\n",
    "- 机器学习---线性回归\n",
    "- 模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归性能分析\n",
    "$$\n",
    "M S E=\\frac{1}{m} \\sum_{i=1}^{m}\\left(y^{i}-\\bar{y}\\right)^{2}\n",
    "$$\n",
    "（注意：yi是预测值，y为真实值）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "api调用：  \n",
    "- sklearn.metrics.mean_squared_error(y_true, y_pred)\n",
    "    - 均⽅误差回归损失\n",
    "    - y_true:真实值\n",
    "    - y_pred:预测值\n",
    "    - return:浮点数结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正规方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个模型的偏置是：\n",
      " 22.81237623762382\n",
      "这个模型的系数是：\n",
      " [-1.20360076  1.03052186  0.4698835   0.47906963 -1.7466153   3.09549361\n",
      " -0.22698449 -3.05932442  2.76146472 -2.30902676 -2.15148921  0.42798269\n",
      " -3.70694201]\n",
      "预测值是：\n",
      " [16.83119988 19.40286752 25.2483401  23.0773477  25.14717104 39.15923928\n",
      " 10.13979422 21.81375998 14.87785666 34.76291739 25.62603336 41.42176003\n",
      " 19.37358879 30.70546662 20.55411695  6.48826428 33.90878768 25.63765953\n",
      " 21.41496983 13.97560701 21.05920101 17.12300809 19.0061098  17.65407697\n",
      " 25.03033241 15.83793008 23.72629459 18.50168512 18.8037346  20.29062717\n",
      " 20.6964419  28.33930224 29.88296412 38.91419252 28.35167827 15.05853625\n",
      " 24.04654791 21.59158934 20.69005457 26.15764458 28.83365489 39.16638021\n",
      " 34.30453821 22.0944951  24.28387348 28.19809582 20.02265043 24.45139565\n",
      " 29.4050658  40.00187763 26.03952492  5.996994   31.53256446 24.27868584\n",
      " 19.33193923 37.99239936 16.36746516 16.41377532 28.44012717 25.83886674\n",
      " 18.7175901  16.24453225 16.09295538 33.34280082 34.26347295 21.69053224\n",
      " 15.13370204 21.78783824 21.71177028 23.63092171 23.13113851 27.22074601\n",
      "  6.47512161 16.30375437 -3.1118775  21.60232861 23.42422333 19.39243223\n",
      " 28.81656875 20.19528564 15.24192313 28.48927672 13.05061865 17.78999052\n",
      " 32.04354774 25.78739506 34.45387589  3.87418763 18.10886853 21.54028507\n",
      " 27.30402289 31.49931551 21.00155518 17.74624532 26.861474   16.07898318\n",
      " 35.91660534 29.4507603  18.5936104  17.35573039 18.36458395  6.92061449]\n",
      "均方差是：\n",
      " 29.591480096296035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression,SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#获取数据\n",
    "boston=load_boston()\n",
    "#print(boston)\n",
    "\n",
    "#数据基本处理---分割数据\n",
    "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target,test_size=0.2)\n",
    "\n",
    "#特征工程---标准化\n",
    "transfer=StandardScaler()\n",
    "x_train=transfer.fit_transform(x_train)\n",
    "x_test=transfer.fit_transform(x_test)\n",
    "\n",
    "#机器学习---线性回归\n",
    "estimator=LinearRegression()\n",
    "estimator.fit(x_train,y_train)\n",
    "\n",
    "print(\"这个模型的偏置是：\\n\",estimator.intercept_)\n",
    "print(\"这个模型的系数是：\\n\",estimator.coef_)\n",
    "\n",
    "#模型评估\n",
    "y_pre=estimator.predict(x_test)\n",
    "print(\"预测值是：\\n\",y_pre)\n",
    "\n",
    "ret=mean_squared_error(y_test,y_pre)\n",
    "print(\"均方差是：\\n\",ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个模型的偏置是：\n",
      " [-14914.82008193]\n",
      "这个模型的系数是：\n",
      " [-1.84582354e+04  2.22671674e+03  6.63420582e+03  1.64454106e+04\n",
      "  2.39183683e+04 -2.56662315e+03 -1.05475786e+04  1.40348944e+04\n",
      " -3.51092356e+04 -1.64726431e+04 -1.55699718e+04 -1.95199631e+04\n",
      "  3.03604711e+01]\n",
      "预测值是：\n",
      " [  38454.697528    -44763.05809783    7395.89951343   37455.95069708\n",
      "   22841.90330686   35155.02766964  -96609.25695224   -1972.03888843\n",
      " -124859.2555308     9477.42678105   -1060.17710647 -148966.47364975\n",
      "   14279.85015934 -127494.41225167    6204.42682814    7783.05386893\n",
      "    2902.78929205   63679.52006146   -8618.58316868   -6838.13850181\n",
      "  -50790.71234192   79949.49695798  -16099.8207738   -25047.37900352\n",
      "    2050.13084705    7065.05639716   99875.00575204    8480.88439721\n",
      " -126499.92087864   47617.61375811   36502.85450898   26987.79175734\n",
      " -211513.03656035   42621.99359699   16143.26836821  -23822.51011239\n",
      "  -16869.07466359  -25345.11103618  -64717.61479554   55023.18627326\n",
      "   19430.56872299 -188797.60606825 -100571.24330958  -19067.07040915\n",
      "   39605.25919199   21507.02442757   27226.86655957 -121783.17865117\n",
      "  -54239.59622047  -32263.81335179  145859.90990404   13123.69111865\n",
      "   19429.55131634  -48853.21170141   13804.76577727 -189943.14705305\n",
      "  -20387.02802379   34614.74748091 -102649.00801789   68496.63723272\n",
      "  -21974.4357154    53359.42967704    4926.2349879   -14081.44718601\n",
      "  -51393.02266731  -19017.28984796   59196.90283916   34699.68605105\n",
      " -204148.40939627   17246.39475296   27422.71750279   33356.64034521\n",
      "  -21465.05268729  -45046.13617795   10543.99956575  -14276.31528839\n",
      " -125964.94112291   -8986.21522246   30791.20507474   86808.85748348\n",
      "  -10951.59407401   21897.96498769 -127483.61771129   39215.92737343\n",
      "    3268.37909043   38530.07211542    1741.76472502   38692.73388276\n",
      "  -12553.98619977  -43006.64989432   34281.14520375 -165783.38674548\n",
      "  -41818.78841362  -49290.17182762  -47879.22538043   10719.20677794\n",
      "    5089.55103929 -139032.81504869  -35256.80517664   18221.73462198\n",
      "   48170.09685786  -10687.36046122]\n",
      "均方差是：\n",
      " 4708300680.015552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#获取数据\n",
    "boston=load_boston()\n",
    "#print(boston)\n",
    "\n",
    "#数据基本处理---分割数据\n",
    "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target,test_size=0.2)\n",
    "\n",
    "#特征工程---标准化\n",
    "transfer=StandardScaler()\n",
    "x_train=transfer.fit_transform(x_train)\n",
    "x_test=transfer.fit_transform(x_test)\n",
    "\n",
    "#机器学习---线性回归（梯度下降）\n",
    "estimator=SGDRegressor(max_iter=1000,learning_rate=\"constant\",eta0=0.1)\n",
    "#estimator=SGDRegressor(max_iter=1000)\n",
    "estimator.fit(x_train,y_train)\n",
    "\n",
    "print(\"这个模型的偏置是：\\n\",estimator.intercept_)\n",
    "print(\"这个模型的系数是：\\n\",estimator.coef_)\n",
    "\n",
    "#模型评估\n",
    "y_pre=estimator.predict(x_test)\n",
    "print(\"预测值是：\\n\",y_pre)\n",
    "\n",
    "ret=mean_squared_error(y_test,y_pre)\n",
    "print(\"均方差是：\\n\",ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过拟合和欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 欠拟合：\n",
    "    - 在训练集上表现不好，在测试集上也表现不好；\n",
    "    - 解决方案：\n",
    "        - 添加其他的特征项\n",
    "        - 添加多项式特征\n",
    "\n",
    "\n",
    "- 过拟合：\n",
    "    - 在训练集上表现好，在测试集上表现不好\n",
    "    - 解决方案：\n",
    "        - 重新清洗数据集\n",
    "        - 增大数据的训练量\n",
    "        - 正则化\n",
    "        - 减少特征维度，防止维灾难\n",
    "\n",
    "\n",
    "- 正则化：\n",
    "    - 定义：通过限制高次项的系数防止过拟合\n",
    "    - L1正则化：直接把高次项系数变为0\n",
    "        - Lassos回归\n",
    "    - L2正则化：把⾼次项前⾯的系数变成特别⼩的值\n",
    "        - Ridge回归（岭回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
