{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;决策树的思想就是程序设计中的分支结构if-else，决策树是一种分类学习方法；\n",
    "- 一种树形结构，本质是一颗由多个判断结点组成的树；\n",
    "- 每个内部结点表示一个属性上的判断；\n",
    "- 每个分支代表一个判断结果的输出；\n",
    "- 叶结点代表分类结果；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树分类原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在物理学上，熵是“混乱”程度的度量。系统越有序，熵值越低，系统越混乱，熵值越高。\n",
    "- （1）从信息的完整性进行描述：\n",
    "    - 当系统有序状态一致时，数据越集中的地方，熵值越小，数据越分散的地方，熵值越大；\n",
    "- （2）从信息的有序性上进行分析：\n",
    "    - 当数据一致时，系统越有序，熵值越低；系统越混乱或者分散时，熵值越高；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵是度量样本集合纯度最常用的一种指标：\n",
    "$$\n",
    "\\operatorname{Ent}(D)=-\\sum_{k=1}^{n} \\frac{C^{k}}{D} \\log \\frac{C^{k}}{D}=-\\sum_{k=1}^{n} p_{k} \\log _{2} p_{k}=-p_{1} \\log _{2} p_{1}-p_{2} \\log _{2} p_{2}-\\ldots-p_{n} \\log _{2} p_{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 决策树的划分依据（1）——信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义：使⽤划分前后集合熵的差值来衡量使⽤当前特征对于样本集合D划分效果的好坏（信息增益 = entroy(前) - entroy(后)）  \n",
    "公式：\n",
    "$$\n",
    "\\operatorname{Gain}(D, a)=\\operatorname{Ent}(D)-\\operatorname{Ent}(D \\mid a)=\\operatorname{Ent}(D)-\\sum_{v=1}^{V} \\frac{D^{v}}{D} \\operatorname{Ent}\\left(D^{v}\\right)\n",
    "$$  \n",
    "(特征a对训练数据集D的信息增益Gain(D,a),定义为集合D的信息熵Ent(D)与给定特征a条件下D的信息条件熵Ent(D∣a)之差)  \n",
    "\n",
    "\n",
    "具体流程：（一般用于的 ID3 决策树学习算法）  \n",
    "- 计算类别信息熵\n",
    "- 计算属性的信息熵\n",
    "- 计算属性信息增益\n",
    "- 做比较\n",
    "\n",
    "ID3 算法缺点：\n",
    "- [x] ID3算法在选择根节点和各内部节点中的分⽀属性时，采⽤信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。\n",
    "- [x] ID3算法只能对描述属性为离散型属性的数据集构造决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  决策树的划分依据（2）——信息增益率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;若将编号也作为一个候选划分属性，再使用信息增益就会有所误差，因为信息增益准则对可取值数⽬较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法不直接使⽤信息增益，⽽是使⽤\"增益率\" (gain ratio) 来选择最优划分属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增益率：\n",
    "$$\n",
    "\\operatorname{Gain}_{-} \\operatorname{ratio}(D, a)=\\frac{\\operatorname{Gain}(D, a)}{I V(a)}\n",
    "$$  \n",
    "其中：\n",
    "$$\n",
    "I V(a)=-\\sum_{v=1}^{V} \\frac{D^{v}}{D} \\log \\frac{D^{v}}{D}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法流程：\n",
    "- 计算当前结点的类别熵（以类别取值计算）；\n",
    "- 计算当前阶段的属性熵(按照属性取值下得类别取值计算)\n",
    "- 计算信息增益\n",
    "- 计算各个属性的分裂信息度量\n",
    "- 计算各个属性的信息增益率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用C4.5决策树的好处：  \n",
    "- 用信息增益率来选择属性\n",
    "- 可以处理连续数值型属性\n",
    "- 采用一种后剪枝方法\n",
    "- 对于缺失值的处理\n",
    "- 产⽣的分类规则易于理解，准确率较⾼\n",
    "\n",
    "缺点：\n",
    "- 在构造树的过程中，需要对数据集进⾏多次的顺序扫描和排序，因⽽导致算法的低效\n",
    "- 此外，C4.5只适合于能够驻留于内存的数据集，当训练集⼤得⽆法在内存容纳时程序⽆法运⾏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的划分依据（3）——基尼值和基尼指数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;CART 决策树使⽤\"基尼指数\" (Gini index)来选择划分属性.(CART 是Classification and Regression Tree的简称，这是⼀种著名的决策树学习算法,分类和回归任务都可⽤)。  \n",
    "基尼值Gini（D）：从数据集D中随机抽取两个样本，其类别标记不⼀致的概率。故，Gini（D）值越⼩，数据集D的纯度越⾼。\n",
    "\n",
    "\n",
    "基尼值：\n",
    "$$\n",
    "\\operatorname{Gini}(D)=\\sum_{k=1}^{|y|} \\sum_{k \\neq k} p_{k} p_{k}^{\\prime}=1-\\sum_{k=1}^{|y|} p_{k}^{2}\n",
    "$$  \n",
    "\n",
    "基尼指数：\n",
    "$$\n",
    "\\text { Gini_index }(D, a)=\\sum_{v=1}^{V} \\frac{D^{v}}{D} \\operatorname{Gini}\\left(D^{v}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法流程：\n",
    "while(当前节点\"不纯\")：  \n",
    "- 1.遍历每个变量的每⼀种分割⽅式，找到最好的分割点\n",
    "- 2.分割成两个节点N1和N2  \n",
    "end while  \n",
    "每个节点⾜够“纯”为⽌\n",
    "（C4.5不⼀定是⼆叉树，但CART⼀定是⼆叉树）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| 名称 | 提出时间 | 分支方式 | 备注 |  \n",
    "| ---- | -------- | -------- | ---- |  \n",
    "| ID3  | 1975    | 信息增益 | ID3只能对离散属性的数据集构成决策树 |\n",
    "| C4.5 | 1993 | 信息增益率 | 优化后解决了ID3分⽀过程中总喜欢偏向选择值较多的属性 |\n",
    "| CART | 1984 | Gini系数 | 可以进⾏分类和回归，可以处理离散属性，也可以处理连续属性 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树变量的两种类型：\n",
    "- （1）数字型（Numeric）：变量类型是整数或浮点数，如前⾯例⼦中的“年收⼊”。⽤“>=”，“>”,“<”或“<=”作为分割条件（排序后，利⽤已有的分割情况，可以优化分割算法的时间复杂度）。\n",
    "- （2） 名称型（Nominal）：类似编程语⾔中的枚举类型，变量只能从有限的选项中选取，⽐如前⾯例⼦中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”，使⽤“=”来分割。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何评估分割点的好坏：  \n",
    "- [x] 如果⼀个分割点可以将当前的所有节点分为两类，使得每⼀类都很“纯”，也就是同⼀类的记录较多，那么就是⼀个好分割点。构建决策树采⽤贪⼼算法，只考虑当前纯度差最⼤的情况作为分割点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
