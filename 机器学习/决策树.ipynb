{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;决策树的思想就是程序设计中的分支结构if-else，决策树是一种分类学习方法；\n",
    "- 一种树形结构，本质是一颗由多个判断结点组成的树；\n",
    "- 每个内部结点表示一个属性上的判断；\n",
    "- 每个分支代表一个判断结果的输出；\n",
    "- 叶结点代表分类结果；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树分类原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在物理学上，熵是“混乱”程度的度量。系统越有序，熵值越低，系统越混乱，熵值越高。\n",
    "- （1）从信息的完整性进行描述：\n",
    "    - 当系统有序状态一致时，数据越集中的地方，熵值越小，数据越分散的地方，熵值越大；\n",
    "- （2）从信息的有序性上进行分析：\n",
    "    - 当数据一致时，系统越有序，熵值越低；系统越混乱或者分散时，熵值越高；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵是度量样本集合纯度最常用的一种指标：\n",
    "$$\n",
    "\\operatorname{Ent}(D)=-\\sum_{k=1}^{n} \\frac{C^{k}}{D} \\log \\frac{C^{k}}{D}=-\\sum_{k=1}^{n} p_{k} \\log _{2} p_{k}=-p_{1} \\log _{2} p_{1}-p_{2} \\log _{2} p_{2}-\\ldots-p_{n} \\log _{2} p_{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 决策树的划分依据（1）——信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义：使⽤划分前后集合熵的差值来衡量使⽤当前特征对于样本集合D划分效果的好坏（信息增益 = entroy(前) - entroy(后)）  \n",
    "公式：\n",
    "$$\n",
    "\\operatorname{Gain}(D, a)=\\operatorname{Ent}(D)-\\operatorname{Ent}(D \\mid a)=\\operatorname{Ent}(D)-\\sum_{v=1}^{V} \\frac{D^{v}}{D} \\operatorname{Ent}\\left(D^{v}\\right)\n",
    "$$  \n",
    "(特征a对训练数据集D的信息增益Gain(D,a),定义为集合D的信息熵Ent(D)与给定特征a条件下D的信息条件熵Ent(D∣a)之差)  \n",
    "\n",
    "\n",
    "具体流程：（一般用于的 ID3 决策树学习算法）  \n",
    "- 计算类别信息熵\n",
    "- 计算属性的信息熵\n",
    "- 计算属性信息增益\n",
    "- 做比较\n",
    "\n",
    "ID3 算法缺点：\n",
    "- [x] ID3算法在选择根节点和各内部节点中的分⽀属性时，采⽤信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。\n",
    "- [x] ID3算法只能对描述属性为离散型属性的数据集构造决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  决策树的划分依据（2）——信息增益率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;若将编号也作为一个候选划分属性，再使用信息增益就会有所误差，因为信息增益准则对可取值数⽬较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法不直接使⽤信息增益，⽽是使⽤\"增益率\" (gain ratio) 来选择最优划分属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增益率：\n",
    "$$\n",
    "\\operatorname{Gain}_{-} \\operatorname{ratio}(D, a)=\\frac{\\operatorname{Gain}(D, a)}{I V(a)}\n",
    "$$  \n",
    "其中：\n",
    "$$\n",
    "I V(a)=-\\sum_{v=1}^{V} \\frac{D^{v}}{D} \\log \\frac{D^{v}}{D}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法流程：\n",
    "- 计算当前结点的类别熵（以类别取值计算）；\n",
    "- 计算当前阶段的属性熵(按照属性取值下得类别取值计算)\n",
    "- 计算信息增益\n",
    "- 计算各个属性的分裂信息度量\n",
    "- 计算各个属性的信息增益率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用C4.5决策树的好处：  \n",
    "- 用信息增益率来选择属性\n",
    "- 可以处理连续数值型属性\n",
    "- 采用一种后剪枝方法\n",
    "- 对于缺失值的处理\n",
    "- 产⽣的分类规则易于理解，准确率较⾼\n",
    "\n",
    "缺点：\n",
    "- 在构造树的过程中，需要对数据集进⾏多次的顺序扫描和排序，因⽽导致算法的低效\n",
    "- 此外，C4.5只适合于能够驻留于内存的数据集，当训练集⼤得⽆法在内存容纳时程序⽆法运⾏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的划分依据（3）——基尼值和基尼指数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;CART 决策树使⽤\"基尼指数\" (Gini index)来选择划分属性.(CART 是Classification and Regression Tree的简称，这是⼀种著名的决策树学习算法,分类和回归任务都可⽤)。  \n",
    "基尼值Gini（D）：从数据集D中随机抽取两个样本，其类别标记不⼀致的概率。故，Gini（D）值越⼩，数据集D的纯度越⾼。\n",
    "\n",
    "\n",
    "基尼值：\n",
    "$$\n",
    "\\operatorname{Gini}(D)=\\sum_{k=1}^{|y|} \\sum_{k \\neq k} p_{k} p_{k}^{\\prime}=1-\\sum_{k=1}^{|y|} p_{k}^{2}\n",
    "$$  \n",
    "\n",
    "基尼指数：\n",
    "$$\n",
    "\\text { Gini_index }(D, a)=\\sum_{v=1}^{V} \\frac{D^{v}}{D} \\operatorname{Gini}\\left(D^{v}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法流程：\n",
    "while(当前节点\"不纯\")：  \n",
    "- 1.遍历每个变量的每⼀种分割⽅式，找到最好的分割点\n",
    "- 2.分割成两个节点N1和N2  \n",
    "end while  \n",
    "每个节点⾜够“纯”为⽌\n",
    "（C4.5不⼀定是⼆叉树，但CART⼀定是⼆叉树）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| 名称 | 提出时间 | 分支方式 | 备注 |  \n",
    "| ---- | -------- | -------- | ---- |  \n",
    "| ID3  | 1975    | 信息增益 | ID3只能对离散属性的数据集构成决策树 |\n",
    "| C4.5 | 1993 | 信息增益率 | 优化后解决了ID3分⽀过程中总喜欢偏向选择值较多的属性 |\n",
    "| CART | 1984 | Gini系数 | 可以进⾏分类和回归，可以处理离散属性，也可以处理连续属性 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树变量的两种类型：\n",
    "- （1）数字型（Numeric）：变量类型是整数或浮点数，如前⾯例⼦中的“年收⼊”。⽤“>=”，“>”,“<”或“<=”作为分割条件（排序后，利⽤已有的分割情况，可以优化分割算法的时间复杂度）。\n",
    "- （2） 名称型（Nominal）：类似编程语⾔中的枚举类型，变量只能从有限的选项中选取，⽐如前⾯例⼦中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”，使⽤“=”来分割。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何评估分割点的好坏：  \n",
    "- [x] 如果⼀个分割点可以将当前的所有节点分为两类，使得每⼀类都很“纯”，也就是同⼀类的记录较多，那么就是⼀个好分割点。构建决策树采⽤贪⼼算法，只考虑当前纯度差最⼤的情况作为分割点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# cart剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么要剪枝\n",
    "&emsp;&emsp;剪枝 (pruning)是决策树学习算法对付\"过拟合\"的主要⼿段。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分⽀过多，这时就可能因训练样本学得\"太好\"了（过拟合）。因此，可通过主动去掉⼀些分⽀来降低过拟合的⻛险。\n",
    "- 噪声、样本冲突，即错误的样本数据\n",
    "- 特征即属性不能完全作为分类标准\n",
    "- 巧合的规律性，数据量不够⼤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常用剪枝方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预剪枝：\n",
    "- 在构建树的同时剪枝；\n",
    "- 限制节点最小样本数\n",
    "- 指定数据高度；\n",
    "- 指定熵值的最小值\n",
    "\n",
    "\n",
    "后剪枝：\n",
    "- 将一个树构建完成后，再进行从下往上的剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 特征工程——特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;将任意数据（如文本或图像）转换为可用于机器学习的数字特征（注：特征化是为了计算机更好去理解数据）。\n",
    "\n",
    "特征提取分类：\n",
    "- 字典特征提取（特征离散化）\n",
    "- 文本特征提取\n",
    "- 图像特征提取（深度学习介绍）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字典特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "api介绍：\n",
    "- sklearn.feature_extraction\n",
    "- sklearn.feature_extraction.DictVectorizer(sparse=True,…)\n",
    "    - DictVectorizer.fit_transform(X)\n",
    "        - X:字典或者包含字典的迭代器返回值\n",
    "        - 返回sparse矩阵\n",
    "    - DictVectorizer.get_feature_names() 返回类别名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "返回结果：\n",
      " [[  0.   1.   0. 100.]\n",
      " [  1.   0.   0.  60.]\n",
      " [  0.   0.   1.  30.]]\n",
      "特征名称：\n",
      " ['city=上海', 'city=北京', 'city=深圳', 'temperature']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#获取数据\n",
    "data = [{'city': '北京','temperature':100}, {'city': '上海','temperature':60}, {'city': '深圳','temperature':30}]\n",
    "\n",
    "#实例化一个转换器\n",
    "transfer = DictVectorizer(sparse=False)\n",
    "#特征训练\n",
    "data=transfer.fit_transform(data)\n",
    "\n",
    "print(\"返回结果：\\n\",data)\n",
    "\n",
    "print(\"特征名称：\\n\",transfer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 英文文本特征提取\n",
    "api介绍：\n",
    "- sklearn.feature_extraction.text.CountVectorizer(stop_words=[])\n",
    "    - 返回词频矩阵\n",
    "    - CountVectorizer.fit_transform(X)\n",
    "        - X:⽂本或者包含⽂本字符串的可迭代对象\n",
    "        - 返回值:返回sparse矩阵\n",
    "    - CountVectorizer.get_feature_names() 返回值:单词列表\n",
    "- sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征名称是：\n",
      " ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']\n",
      "[[0 1 1 2 0 1 1 0]\n",
      " [1 1 1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#获取数据\n",
    "data = [\"life is short,i like like python\", \"life is too long,i dislike python\"]\n",
    "#实例化转换器\n",
    "transfer=CountVectorizer()\n",
    "#文本特征转换\n",
    "newdata=transfer.fit_transform(data)\n",
    "\n",
    "#查看特征名称\n",
    "names=transfer.get_feature_names()\n",
    "print(\"特征名称是：\\n\",names)\n",
    "print(newdata.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文文本特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为CountVetorizer只能根据空格分词，所以当引入中文时，无法进行分词！\n",
    "解决方法：\n",
    "- 准备句子，利用jieba.cut进行分词\n",
    "- 实例化CountVetorizer\n",
    "- 将分词结果变成字符串当作fit_transform的输入值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\老大\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.578 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['⼀ 种 还是 ⼀ 种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 ⼤ 部分 是 死 在 明天 晚上 ， 所以 每个 ⼈ 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 ⼏ 百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只 ⽤ ⼀ 种 ⽅ 式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']\n",
      "文本特征提取结果：\n",
      " [[0 1 0 0 0 2 0 0 0 1 0 0 0 0 0 1 1 2 0 1 0 2 1 0 0 0 0 1 1 0 0 1 0 1]\n",
      " [0 0 1 0 0 0 1 1 0 0 0 0 0 1 3 0 0 0 1 0 0 0 0 1 2 0 0 0 0 0 1 0 1 0]\n",
      " [1 0 0 4 3 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 2 1 0 0 1 0 0 0 0]]\n",
      "返回特征名字：\n",
      " ['不会', '不要', '之前', '了解', '事物', '今天', '光是在', '发出', '取决于', '后天', '含义', '如何', '如果', '宇宙', '我们', '所以', '放弃', '明天', '星系', '晚上', '某样', '残酷', '每个', '百万年', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样', '部分']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "#对中文进行分词\n",
    "def cut_word(text):\n",
    "    # ⽤结巴对中⽂字符串进⾏分词\n",
    "    text = \" \".join(list(jieba.cut(text)))\n",
    "    return text\n",
    "\n",
    "#获取数据\n",
    "data = [\"⼀种还是⼀种今天很残酷，明天更残酷，后天很美好，但绝对⼤部分是死在明天晚上，所以每个⼈不要放弃今天。\",\n",
    "        \"我们看到的从很远星系来的光是在⼏百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\",\n",
    "        \"如果只⽤⼀种⽅式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\"]\n",
    "#将原始数据转换成分好词的形式\n",
    "text_list=[]\n",
    "for sent in data:\n",
    "    text_list.append(cut_word(sent))\n",
    "print(text_list)\n",
    "\n",
    "#实例化一个转换器(⾥⾯依旧可以使⽤停⽤词，进⾏词语的限制)\n",
    "transfer=CountVectorizer()\n",
    "#调用\n",
    "data=transfer.fit_transform(text_list)\n",
    "print(\"文本特征提取结果：\\n\",data.toarray())\n",
    "print(\"返回特征名字：\\n\",transfer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf文本特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要思想：\n",
    "- 如果某个词或短语在⼀篇⽂章中出现的概率⾼，并且在其他⽂章中很少出现，则认为此词或者短语具有很好的\n",
    "- 类别区分能⼒，适合⽤来分类\n",
    "\n",
    "\n",
    "ti-idf分别是什么意思：\n",
    "- tf -- 词频\n",
    "- idf -- 逆向⽂档频率\n",
    "\n",
    "api：\n",
    "- sklearn.feature_extraction.text.TfidfVectorizer  \n",
    "（注意：分类机器学习算法进⾏⽂章分类中前期数据处理⽅式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['⼀ 种 还是 ⼀ 种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 ⼤ 部分 是 死 在 明天 晚上 ， 所以 每个 ⼈ 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 ⼏ 百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只 ⽤ ⼀ 种 ⽅ 式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']\n",
      "⽂本特征抽取的结果：\n",
      " [[0.         0.         0.         0.43643578 0.         0.\n",
      "  0.         0.21821789 0.         0.         0.         0.\n",
      "  0.         0.21821789 0.21821789 0.43643578 0.         0.21821789\n",
      "  0.         0.43643578 0.21821789 0.         0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.         0.21821789\n",
      "  0.         0.21821789]\n",
      " [0.2410822  0.         0.         0.         0.2410822  0.2410822\n",
      "  0.         0.         0.         0.         0.         0.2410822\n",
      "  0.55004769 0.         0.         0.         0.2410822  0.\n",
      "  0.         0.         0.         0.2410822  0.48216441 0.\n",
      "  0.         0.         0.         0.         0.2410822  0.\n",
      "  0.2410822  0.        ]\n",
      " [0.         0.6613748  0.4960311  0.         0.         0.\n",
      "  0.1653437  0.         0.1653437  0.1653437  0.1653437  0.\n",
      "  0.12574815 0.         0.         0.         0.         0.\n",
      "  0.1653437  0.         0.         0.         0.         0.3306874\n",
      "  0.1653437  0.         0.         0.1653437  0.         0.\n",
      "  0.         0.        ]]\n",
      "返回特征名字：\n",
      " ['之前', '了解', '事物', '今天', '光是在', '发出', '取决于', '后天', '含义', '如何', '如果', '宇宙', '我们', '所以', '放弃', '明天', '星系', '晚上', '某样', '残酷', '每个', '百万年', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样', '部分']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#获取数据\n",
    "data = [\"⼀种还是⼀种今天很残酷，明天更残酷，后天很美好，但绝对⼤部分是死在明天晚上，所以每个⼈不要放弃今天。\",\n",
    "        \"我们看到的从很远星系来的光是在⼏百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\",\n",
    "        \"如果只⽤⼀种⽅式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\"]\n",
    "# 将原始数据转换成分好词的形式\n",
    "text_list = []\n",
    "for sent in data:\n",
    "    text_list.append(cut_word(sent))\n",
    "print(text_list)\n",
    "# 1、实例化⼀个转换器类\n",
    "transfer = TfidfVectorizer(stop_words=['⼀种', '不会', '不要'])\n",
    "# 2、调⽤fit_transform\n",
    "data = transfer.fit_transform(text_list)\n",
    "print(\"⽂本特征抽取的结果：\\n\", data.toarray())\n",
    "print(\"返回特征名字：\\n\", transfer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
